{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "session_02.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN3/P7eM3nSPfyElJYqbVUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrgeo/ml-tutorials/blob/main/session_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK5Yv3qVfprN"
      },
      "source": [
        "# Session 2: Salt classification with CNNs using TensorFlow and Keras\n",
        "\n",
        "In this session, you will learn how to **train a simple CNN using TensorFlow and Keras**, for the purpose of salt classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDS4G-2VqNPA"
      },
      "source": [
        "\n",
        "As usual, we start by importing the necessary python packages. If you participated in session one, you should already be familiar with numpy and matplotlib. \n",
        "\n",
        "Additionaly, in this session we import **TensorFlow**, which is a free and **open-source machine learning library**. By default, TensorFlow includes a library called **Keras** which is used as an underlying API for the purpose of **designing and training Deep Neural Networks**. You can learn more about TensorFlow and also find additional tutorials and guides on their [website](https://www.tensorflow.org/overview)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1xhoLvboNh0"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp6F4GvSgLys"
      },
      "source": [
        "## 1. Preparing the Datasets\n",
        "For this session, we are providing a **dataset of 28x28 pixel image patches**, extracted from the Z3 Netherlands seismic survey. Each image has been **labelled as 'salt' or 'background'**. We split the data into **training and test datasets**, the latter one will later allow us to validate our trained model.\n",
        "Download the dataset from our github repository, by executing the code cell below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49lW6XE5ph9F"
      },
      "source": [
        "!wget https://github.com/vrgeo/ml-tutorials/blob/5c17fe8c779b0ab22243ef77753e94e923dd7deb/data/dataset_train.npz?raw=true -nv -O dataset_train.npz\n",
        "!wget https://github.com/vrgeo/ml-tutorials/blob/5c17fe8c779b0ab22243ef77753e94e923dd7deb/data/dataset_test.npz?raw=true -nv -O dataset_test.npz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4f_1vS-ueey"
      },
      "source": [
        "You might have notized these files have an unusual file extension,.npz. These files were created with numpy's **np.savez()** function, which allows you to write one or more numpy arrays from your running python application into a zipped file.\n",
        "\n",
        "We can now load the files back into python, using the **numpy.load()** function. There are two arrays in each file, which can be accessed using the keys *'arr_0'* and *'arr_1'*. The first array in each file contains the patches, the second one the corresponding labels.\n",
        "\n",
        "Run the cell below in order to load the datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h6n5T82evsi",
        "collapsed": true
      },
      "source": [
        "training_data = np.load(\"dataset_train.npz\", allow_pickle=True)\n",
        "training_patches = training_data[\"arr_0\"]\n",
        "training_labels = training_data[\"arr_1\"]\n",
        "print(f'Loaded {len(training_patches)} training patches and {len(training_labels)} corresponding labels')\n",
        "\n",
        "test_data = np.load(\"dataset_test.npz\", allow_pickle=True)\n",
        "test_patches = test_data[\"arr_0\"]\n",
        "test_labels = test_data[\"arr_1\"]\n",
        "print(f'Loaded {len(test_patches)} training patches and {len(test_labels)} corresponding labels')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4IrKr5cw3cS"
      },
      "source": [
        "Let us begin by examining the training data. First, we shall see how many patches we have of each class. In the dataset, patches are labelled either as **'0' - 'background'** or **'1'- 'salt'**.\n",
        "Using the numpy's **count_nonzero()** function, we count the number of non-zero values in the labels array, thus getting the number of patches labelled as 'salt'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylZ4BezzeaM6",
        "collapsed": true
      },
      "source": [
        "n_salt = np.count_nonzero(training_labels)\n",
        "n_background = len(training_labels) - n_salt\n",
        "print(f'Found {n_salt} patches containing salt and {n_background} patches not containing salt.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d9XWnEzxoz0"
      },
      "source": [
        "We have a **2:1 split of background vs salt**. In image classification, is often advisable to have more examples for the background class, since it usually is more varied than the target class.\n",
        "\n",
        "In the cell below, we define a small python function, that will help us **visualize patches and corresponding labels**, with the help of **matplotlib**. We are going to use it during the rest of the session, so make sure to run the code cell below once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41L9fv_wQibW"
      },
      "source": [
        "def visualize_patches(patches, labels, rows, cols, colormap = 'gray'):\n",
        "  fig = plt.figure(figsize=(cols*2, rows*2.2))\n",
        "  for i in range(len(patches)):\n",
        "    title = labels[i]\n",
        "    plt.subplot(rows, cols, i+1)\n",
        "    plt.imshow(patches[i], cmap = colormap)\n",
        "    plt.axis('off')\n",
        "    plt.title(title)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2_B1xNty6x7"
      },
      "source": [
        "By running the cell below, you can **visualize a random selection of 'salt' patches** from the trainig dataset. We use numpy to get the indices of all patches which have label that is non-zero. We then use numpy's **random.choice()** function to draw twelve random indices from this set. We pass this subset of our dataset to our **visualize_patches()** function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OUZ2YaSM5SH",
        "collapsed": true
      },
      "source": [
        "salt_indices = np.nonzero(training_labels)[0]\n",
        "random_salt_indices = np.random.choice(salt_indices, 12, replace=False)\n",
        "visualize_patches(training_patches[random_salt_indices], training_labels[random_salt_indices], 3, 4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwSZzwqtz-Qy"
      },
      "source": [
        "Similarly, you can run the cell below, in order to **visualize twelve random 'background' patches**. \n",
        "\n",
        "Notice how we inverted the labels before passing them to numpy's nonzero() function. This way, we can use the same method as before to access background paches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFH64qBvdmyR",
        "collapsed": true
      },
      "source": [
        "background_indices = np.nonzero(1-training_labels)[0]\n",
        "random_background_indices = np.random.choice(background_indices, 12, replace=False)\n",
        "visualize_patches(training_patches[random_background_indices], training_labels[random_background_indices], 3, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBCeTP2r01n6"
      },
      "source": [
        "We are now almost ready to design and train our network, but first our **dataset needs to be normalized and reshaped**. Normalizing your dataset to a range between 0 and 1 is always desirable when training machine learning models. The reshaping is necessary for keras to be able to read our data.\n",
        "\n",
        "Run the cell below to normalize and reshape the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVNh1pBegBPA"
      },
      "source": [
        "def normalize_and_reshape_data(input_patches):\n",
        "  patches = tf.keras.utils.normalize(input_patches, axis=1)\n",
        "  patches = np.array(patches).reshape(-1, 28, 28, 1)\n",
        "  return patches\n",
        "\n",
        "training_patches_normalized = normalize_and_reshape_data(training_patches)\n",
        "input_shape =  training_patches_normalized.shape[1:]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwSByYC3glWf"
      },
      "source": [
        "## 2. Defining and training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cTxWXJp2Qi_"
      },
      "source": [
        "Now we can start right away with **designing our network**. The example model in this session is based on **LeNet-5**, one of the earliest CNNs, designed by [LeCun et. al., 1989](https://direct.mit.edu/neco/article/1/4/541/5515/Backpropagation-Applied-to-Handwritten-Zip-Code). It was originally desighned to recognize hand written digits, but has been adapted to perform binary classification for this session.\n",
        "\n",
        "First, we initialize a **sequential model**. This type of Neural Network model is ordered in **layers**, and the output of each layer will be the input of the next layer. The first layer's input will be our 28x28 seismic patches.\n",
        "\n",
        "Because we want to create a **Convolutional Neural Network** (CNN), we start with a **convolution layer**. Convolution layers are the defining feature of CNNs, they **allow the network to learn filters** and thus detect certain features, such as shapes or textures. In keras, we can define the **number of filters** and the **size of the filter kernels** for each convolution layer.\n",
        "\n",
        "Each convolution layer is usually followed by a **pooling layer**. This layer reduces the size of the output of the previous layer, by a **downsampling** method. In this case, four input values are reduced to a single output value by averaging. By adding pooling layers, we force the model to **encode the information** and focus on the significant parts.\n",
        "\n",
        "We add a second pair of convolution and pooling layer. Only by **chaining convolution layers** like this, we enable our model to **learn more complex features** by combining features from a previous conv layer. If a convolution layer has learned to detect edges, then following convolution layer might combine these detections in order to detect shapes or whole objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OA46rbhlOPG"
      },
      "source": [
        "input_shape =  training_patches_normalized.shape[1:]\n",
        "\n",
        "model = tf.keras.models.Sequential()  \n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=6 ,kernel_size=5, strides=1, activation=tf.nn.relu, input_shape=input_shape))\n",
        "model.add(tf.keras.layers.AveragePooling2D(pool_size=2,strides=2))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=16 ,kernel_size=5, strides=1, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.AveragePooling2D(pool_size=2,strides=2))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DriAsuPV-Qz4"
      },
      "source": [
        "After the last convolution and pooling layer follows the classification part of a CNN. This is a **Fully Connected Neural Network** that will take the higher level feature detections from the final convolution layer as an input vector and learns to **classify the feature vectors into 'salt' and 'background'**.\n",
        "\n",
        "Notice how the **final layer has a single output neuron**. It takes the weighted sum of all the neuron activations of the previous layer into a **sigmoid function**, in order to compute a **'salt likelihood'** from 0 to 1 for a given input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TcqbxApmQxM"
      },
      "source": [
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(120, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(84, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba9HOfvLB9Tf"
      },
      "source": [
        "Now we compile and train our model, using **binary crossentropy loss** function, which computes a loss based on how close the final 'salt likelihood' output of the model was to the ground truth label.\n",
        "\n",
        "We train our model for a total of **ten epochs**, during each epoch the entire training dataset is passed through the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn1mV1ptm3D3",
        "collapsed": true,
        "outputId": "05ff29a4-f97c-467d-af42-c4af9a9a73f2"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',  \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history=model.fit(training_patches_normalized, training_labels, epochs=10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "101/101 [==============================] - 2s 14ms/step - loss: 0.2447 - accuracy: 0.8989\n",
            "Epoch 2/10\n",
            "101/101 [==============================] - 1s 13ms/step - loss: 0.2357 - accuracy: 0.9010\n",
            "Epoch 3/10\n",
            "101/101 [==============================] - 1s 13ms/step - loss: 0.2255 - accuracy: 0.9032\n",
            "Epoch 4/10\n",
            "101/101 [==============================] - 1s 13ms/step - loss: 0.2259 - accuracy: 0.9103\n",
            "Epoch 5/10\n",
            "101/101 [==============================] - 1s 13ms/step - loss: 0.2116 - accuracy: 0.9150\n",
            "Epoch 6/10\n",
            "101/101 [==============================] - 1s 13ms/step - loss: 0.2138 - accuracy: 0.9128\n",
            "Epoch 7/10\n",
            "101/101 [==============================] - 1s 13ms/step - loss: 0.1990 - accuracy: 0.9206\n",
            "Epoch 8/10\n",
            "101/101 [==============================] - 1s 13ms/step - loss: 0.1885 - accuracy: 0.9218\n",
            "Epoch 9/10\n",
            "101/101 [==============================] - 1s 13ms/step - loss: 0.1791 - accuracy: 0.9302\n",
            "Epoch 10/10\n",
            "101/101 [==============================] - 1s 13ms/step - loss: 0.1653 - accuracy: 0.9358\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22yEGFrlMlqI"
      },
      "source": [
        "Since we **saved the training logs as an object** called history, we can now **plot the development of the loss during training**, using matplotlib. From the grapgh, we can see that the **loss has not reached a plateau yet**, so training for more than ten epochs might improve the model further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKLzs8mOME9r"
      },
      "source": [
        "plt.plot(history.history['loss'], label='Binary Cross Entropy Loss')\n",
        "plt.ylabel('BCE Loss value')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJnBQ6Uvgz2t"
      },
      "source": [
        "## 3. Evaluation\n",
        "Now that we have trained our model, we can **validate it's performance** on our test dataest. \n",
        "\n",
        "As before, we need to **reshape and normalize** the data first. After that, we can do inference by simply calling **model.predict()** on our normalized data. We get back salt likelyhood predictions, which we **binarize to 0 and 1**, in order to compare them to the ground truth labels.\n",
        "\n",
        "Using a helpful **function from Scikit-Learn, called classification_report()**, we can get a detailed report on the performance of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhLI3Hk1qIqQ"
      },
      "source": [
        "test_patches_normalized = normalize_and_reshape_data(test_patches)\n",
        "\n",
        "predictions = model.predict(test_patches_normalized)\n",
        "prediction_values = predictions.reshape(-1)\n",
        "prediction_labels = (prediction_values > 0.5).astype(np.uint8)\n",
        "\n",
        "print(classification_report(test_labels, prediction_labels, digits=5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4TeqatfH_35"
      },
      "source": [
        "In the resultig table, we can see **several metrics**, that descibe the performance of our mode. \n",
        "\n",
        "**Accuracy** simply describes how many patches were classified correctly. \n",
        "\n",
        "For a given class, **precision** describes how many of the detections for that class actually belonged to that class. If this value is low, there are many **false positives**. \n",
        "\n",
        "**Recall** on the other hand, describes how many of the true instances of a class were correctly detected. If the recall is low, the model prodices a lot of **false negatives**. \n",
        "\n",
        "You have to consider both precision and recall when evaluating the performance of a model, which is why the **F1-score** is commonly used as a metric, since it is a **function of both precision and recall**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGzeY0k5Jxoh"
      },
      "source": [
        "Using the cell beow, you can **visualize a random subset of the training patches**, along with the model's **salt likelyhood prediction**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr183lHht6aG"
      },
      "source": [
        "random_test_indices = np.random.choice(np.arange(len(test_patches)), 12, replace=False)\n",
        "visualize_patches(test_patches[random_test_indices], prediction_values[random_test_indices], 3, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNh3uGugKW3k"
      },
      "source": [
        "Using the cell below, you can then **view the ground truth labels** of the same patches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0I2ZXH5vcHz"
      },
      "source": [
        "visualize_patches(test_patches[random_test_indices], test_labels[random_test_indices], 3, 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDCi_qo7KpoT"
      },
      "source": [
        "In the cell below, we want to **visualize the falsely classified patches** specifically. We achieve this, by **computing the residuals**, the difference between predicted and true label. We access the instances where this value is not zero. \n",
        "\n",
        "Run the cell below in order to visualize the wrongly classified patches and their corresponding salt likelihood value returned by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiJxy194v9iX"
      },
      "source": [
        "residuals = test_labels - prediction_labels\n",
        "missclassified_indices = np.nonzero(residuals)[0]\n",
        "\n",
        "visualize_patches(test_patches[missclassified_indices], prediction_values[missclassified_indices], 1+(len(missclassified_indices)/4), 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdQRnGIhN4Bd"
      },
      "source": [
        "##Thank you for participating!\n",
        "This is it for Session 2 of our ML tutorials. We hope you learned something new about training CNN classifiers using TensorFlow and Keras, as well as general machine learning best-practices, such as evaluating different metrics, inspecting the loss graph of your model and looking at missclassified instances.\n",
        "\n",
        "## Where to go from here?\n",
        "If you wanted to further improve the model from this session, there could be several ways. We have seen from the loss funcion, that training for more epochs could have a beneficial effect. \n",
        "\n",
        "The model used here is also a rather simple one, why don't you have a try at adding additioinal convolutional and pooling layers, or more fully connected layers at the end and see how that chages the performance? It should be easy to design and test your own CNN models within this notebook.\n",
        "\n",
        "Finally, you might be able to import your own labelled dataset into this notebook. You do not have to use the same method of loading npz files, the images could also be .pngs from your harddrive (colab allows you to upload data from your pc) or even slices loaded from a segy file (see session 1).\n",
        "\n"
      ]
    }
  ]
}